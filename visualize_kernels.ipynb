{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/protagora/learnable-activation-function/blob/dev/visualize_kernels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 628
        },
        "id": "7IcjFH5JFEOc",
        "outputId": "3503fe79-7783-46ca-bb66-e329eaa90abd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241113_121545-9oem0wkf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/aneuralresearch/resnet34-cifar10/runs/9oem0wkf' target=\"_blank\">wise-paper-8</a></strong> to <a href='https://wandb.ai/aneuralresearch/resnet34-cifar10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/aneuralresearch/resnet34-cifar10' target=\"_blank\">https://wandb.ai/aneuralresearch/resnet34-cifar10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/aneuralresearch/resnet34-cifar10/runs/9oem0wkf' target=\"_blank\">https://wandb.ai/aneuralresearch/resnet34-cifar10/runs/9oem0wkf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/aneuralresearch/resnet34-cifar10/runs/9oem0wkf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7e76ff051030>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import wandb\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"resnet34-cifar10\", config={\"epochs\": 10, \"batch_size\": 64, \"learning_rate\": 0.001})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NSYpApEaE0PM",
        "outputId": "58b145a0-092a-42e8-d0da-b2bc64a61631"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:15<00:00, 11.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 100] Loss: 2.0762\n",
            "[Epoch 1, Batch 200] Loss: 1.7946\n",
            "[Epoch 1, Batch 300] Loss: 1.7182\n",
            "[Epoch 1, Batch 400] Loss: 1.6455\n",
            "[Epoch 1, Batch 500] Loss: 1.5295\n",
            "[Epoch 1, Batch 600] Loss: 1.4640\n",
            "[Epoch 1, Batch 700] Loss: 1.4463\n",
            "[Epoch 2, Batch 100] Loss: 1.3949\n",
            "[Epoch 2, Batch 200] Loss: 1.3253\n",
            "[Epoch 2, Batch 300] Loss: 1.3106\n",
            "[Epoch 2, Batch 400] Loss: 1.2561\n",
            "[Epoch 2, Batch 500] Loss: 1.2302\n",
            "[Epoch 2, Batch 600] Loss: 1.2068\n",
            "[Epoch 2, Batch 700] Loss: 1.1725\n",
            "[Epoch 3, Batch 100] Loss: 1.1568\n",
            "[Epoch 3, Batch 200] Loss: 1.1492\n",
            "[Epoch 3, Batch 300] Loss: 1.1146\n",
            "[Epoch 3, Batch 400] Loss: 1.0698\n",
            "[Epoch 3, Batch 500] Loss: 1.0642\n",
            "[Epoch 3, Batch 600] Loss: 1.0820\n",
            "[Epoch 3, Batch 700] Loss: 1.0252\n",
            "[Epoch 4, Batch 100] Loss: 1.0527\n",
            "[Epoch 4, Batch 200] Loss: 0.9922\n",
            "[Epoch 4, Batch 300] Loss: 0.9799\n",
            "[Epoch 4, Batch 400] Loss: 0.9663\n",
            "[Epoch 4, Batch 500] Loss: 0.9612\n",
            "[Epoch 4, Batch 600] Loss: 0.9494\n",
            "[Epoch 4, Batch 700] Loss: 0.9500\n",
            "[Epoch 5, Batch 100] Loss: 0.8875\n",
            "[Epoch 5, Batch 200] Loss: 0.8920\n",
            "[Epoch 5, Batch 300] Loss: 0.8875\n",
            "[Epoch 5, Batch 400] Loss: 0.8919\n",
            "[Epoch 5, Batch 500] Loss: 0.9000\n",
            "[Epoch 5, Batch 600] Loss: 0.8783\n",
            "[Epoch 5, Batch 700] Loss: 0.9295\n",
            "[Epoch 6, Batch 100] Loss: 0.8658\n",
            "[Epoch 6, Batch 200] Loss: 0.8351\n",
            "[Epoch 6, Batch 300] Loss: 0.8452\n",
            "[Epoch 6, Batch 400] Loss: 0.8343\n",
            "[Epoch 6, Batch 500] Loss: 0.8603\n",
            "[Epoch 6, Batch 600] Loss: 0.8376\n",
            "[Epoch 6, Batch 700] Loss: 0.8103\n",
            "[Epoch 7, Batch 100] Loss: 0.7873\n",
            "[Epoch 7, Batch 200] Loss: 0.7754\n",
            "[Epoch 7, Batch 300] Loss: 0.7861\n",
            "[Epoch 7, Batch 400] Loss: 0.7598\n",
            "[Epoch 7, Batch 500] Loss: 0.7474\n",
            "[Epoch 7, Batch 600] Loss: 0.8138\n",
            "[Epoch 7, Batch 700] Loss: 0.7533\n",
            "[Epoch 8, Batch 100] Loss: 0.7364\n",
            "[Epoch 8, Batch 200] Loss: 0.7293\n",
            "[Epoch 8, Batch 300] Loss: 0.7582\n",
            "[Epoch 8, Batch 400] Loss: 0.7065\n",
            "[Epoch 8, Batch 500] Loss: 0.7270\n",
            "[Epoch 8, Batch 600] Loss: 0.6990\n",
            "[Epoch 8, Batch 700] Loss: 0.7082\n",
            "[Epoch 9, Batch 100] Loss: 0.7259\n",
            "[Epoch 9, Batch 200] Loss: 0.7102\n",
            "[Epoch 9, Batch 300] Loss: 0.7119\n",
            "[Epoch 9, Batch 400] Loss: 0.6725\n",
            "[Epoch 9, Batch 500] Loss: 0.7032\n",
            "[Epoch 9, Batch 600] Loss: 0.6994\n",
            "[Epoch 9, Batch 700] Loss: 0.6544\n",
            "[Epoch 10, Batch 100] Loss: 0.6263\n",
            "[Epoch 10, Batch 200] Loss: 0.6501\n",
            "[Epoch 10, Batch 300] Loss: 0.6549\n",
            "[Epoch 10, Batch 400] Loss: 0.6630\n",
            "[Epoch 10, Batch 500] Loss: 0.6715\n",
            "[Epoch 10, Batch 600] Loss: 0.6581\n",
            "[Epoch 10, Batch 700] Loss: 0.6514\n",
            "Test Accuracy: 78.75%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>loss</td><td>██▇▆▆▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>loss</td><td>0.6514</td></tr><tr><td>test_accuracy</td><td>78.75</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wise-paper-8</strong> at: <a href='https://wandb.ai/aneuralresearch/resnet34-cifar10/runs/9oem0wkf' target=\"_blank\">https://wandb.ai/aneuralresearch/resnet34-cifar10/runs/9oem0wkf</a><br/> View project at: <a href='https://wandb.ai/aneuralresearch/resnet34-cifar10' target=\"_blank\">https://wandb.ai/aneuralresearch/resnet34-cifar10</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 20 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20241113_121545-9oem0wkf/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.models import resnet34\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Configuration\n",
        "config = wandb.config\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformations\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# Define model, loss function, and optimizer\n",
        "model = resnet34(pretrained=False, num_classes=10)\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "\n",
        "def log_kernels_to_wandb(layer, layer_name, epoch):\n",
        "    kernels = layer.weight.data.cpu().numpy()\n",
        "    num_kernels = kernels.shape[0]\n",
        "    num_channels = kernels.shape[1]\n",
        "\n",
        "    # Normalize kernels to [0, 1] for visualization\n",
        "    kernels = (kernels - kernels.min()) / (kernels.max() - kernels.min())\n",
        "\n",
        "    # Calculate grid size\n",
        "    num_cols = int(np.ceil(np.sqrt(num_kernels)))\n",
        "    num_rows = int(np.ceil(num_kernels / num_cols))\n",
        "\n",
        "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n",
        "    axes = axes.flatten()  # Flatten to easily iterate and avoid index issues\n",
        "\n",
        "    for i in range(num_kernels):\n",
        "        kernel = kernels[i]\n",
        "        if num_channels == 3:  # RGB kernel\n",
        "            kernel = kernel.transpose(1, 2, 0)\n",
        "        axes[i].imshow(kernel, cmap=\"viridis\")\n",
        "        axes[i].axis(\"off\")\n",
        "\n",
        "    # Turn off any remaining empty subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        axes[j].axis(\"off\")\n",
        "\n",
        "    fig.suptitle(f\"{layer_name} Kernels at Epoch {epoch}\")\n",
        "    wandb.log({f\"{layer_name}_kernels\": wandb.Image(fig)})\n",
        "    plt.close(fig)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(config.epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Log every 100 mini-batches\n",
        "        if i % 100 == 99:\n",
        "            avg_loss = running_loss / 100\n",
        "            wandb.log({\"epoch\": epoch + 1, \"loss\": avg_loss})\n",
        "            print(f\"[Epoch {epoch+1}, Batch {i+1}] Loss: {avg_loss:.4f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Save and log kernels for low-level and high-level features\n",
        "    if epoch % 1 == 0:  # Log every epoch\n",
        "        log_kernels_to_wandb(model.conv1, \"Low_Level_Features\", epoch + 1)\n",
        "        log_kernels_to_wandb(model.layer3[0].conv1, \"High_Level_Features\", epoch + 1)  # Example of a high-level layer\n",
        "\n",
        "# Testing the model and logging accuracy\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in testloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "wandb.log({\"test_accuracy\": accuracy})\n",
        "print(\"Test Accuracy: {:.2f}%\".format(accuracy))\n",
        "\n",
        "wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}